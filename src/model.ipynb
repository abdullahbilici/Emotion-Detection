{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from utils import *\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_path = \"../data/data.npy\"\n",
    "real_data = np.load(real_data_path)\n",
    "\n",
    "ai_data_path = \"../data/data_ai.npy\"\n",
    "ai_data = np.load(ai_data_path)\n",
    "\n",
    "combined_data = np.concatenate([real_data, ai_data], axis = 0)\n",
    "\n",
    "# we apply stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_data[:,:-1], combined_data[:,-1], test_size=0.2, stratify=combined_data[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader([X_train, y_train], shuffle=True, batch_size=8, shape=[128,128], device=DEVICE)\n",
    "test_data = DataLoader([X_test, y_test], shape=[128,128], device=DEVICE)\n",
    "\n",
    "real_data = DataLoader(\"../data/data.npy\", shape=[128,128], device=DEVICE)\n",
    "ai_data = DataLoader(\"../data/data_ai.npy\", shape=[128,128], device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# huge_data = pd.read_csv(\"../data/fer2013.csv\")\n",
    "\n",
    "# huge_data = huge_data[huge_data.emotion.isin([0, 3, 4, 5])].drop([\"Usage\"], axis = 1)\n",
    "# map_labels = {0:0, 3:1, 4:2, 5:3}\n",
    "# huge_data.emotion = huge_data.emotion.map(map_labels)\n",
    "\n",
    "# num_rows_to_drop = int(0.75 * len(huge_data))\n",
    "# rows_to_drop = np.random.choice(huge_data.index, num_rows_to_drop, replace=False)\n",
    "# huge_data = huge_data.drop(index=rows_to_drop)\n",
    "\n",
    "\n",
    "# huge_data.pixels = huge_data.pixels.apply(lambda x: np.array(x.split(\" \")))\n",
    "\n",
    "\n",
    "\n",
    "# huge_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huge = np.concatenate([np.stack(huge_data.to_numpy()[:,1]).astype(\"float\") / 255, huge_data.to_numpy()[:,0].reshape(-1,1)], axis = 1).astype(\"float\")\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(huge[:,:-1], huge[:,-1], test_size=0.2, stratify=huge[:,-1])\n",
    "\n",
    "# train_data = DataLoader([X_train, y_train], shuffle=True, batch_size=64, shape=[48,48], device=DEVICE)\n",
    "# test_data = DataLoader([X_test, y_test], shape=[48,48], device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we had to keep it in different file so we can use it in live test folder \n",
    "from model import *\n",
    "    \n",
    "model = CNN128()\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, C, W, H = train_data.shape\n",
    "N_test = test_data.size\n",
    "\n",
    "n_epoch = 100\n",
    "\n",
    "model = CNN128()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 5e-3)\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "losses = {\"train\": list(), \"test\": list()}\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    model.train()\n",
    "    running_loss = .0\n",
    "    for X_batch, y_batch in train_data:\n",
    "\n",
    "        y_pred = model(X_batch.float())\n",
    "\n",
    "        loss = criterion(y_pred, y_batch.long())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss = loss.item()\n",
    "    \n",
    "    running_loss /= N\n",
    "\n",
    "\n",
    "    # test the model\n",
    "    model.eval()\n",
    "    predictions = model(test_data.X.float())\n",
    "    pred_class = torch.argmax(predictions, axis = 1)\n",
    "    test_acc = (pred_class == test_data.y).float().mean()\n",
    "    test_loss = criterion(predictions, test_data.y.long()).item() / N_test\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1:>2}, Train loss: \\033[92m{running_loss:.4f}\\033[0m, Test loss: \\033[94m{test_loss:.4f}\\033[0m, Test accuracy: {colorize_accuracy(test_acc)}{test_acc:.4f}\\033[0m.\")\n",
    "\n",
    "    # save the best model\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss == test_loss\n",
    "        best_model = model.state_dict()\n",
    "\n",
    "    losses[\"train\"].append(running_loss)\n",
    "    losses[\"test\"].append(test_loss)\n",
    "\n",
    "\n",
    "# load the best model\n",
    "model.load_state_dict(best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_map = {0.:\"angry\", 1.:\"happy\", 2.:\"sad\", 3.:\"shocked\"}\n",
    "\n",
    "predictions = model(test_data.X.float()).argmax(axis = 1)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "\n",
    "axs = axs.flatten()\n",
    "# Loop through each image and plot it\n",
    "for i in range(test_data.size):\n",
    "    if i == 25:\n",
    "        break\n",
    "    axs[i].imshow(test_data.X[i].cpu().squeeze(), cmap='gray')  # Assuming grayscale images\n",
    "    axs[i].axis('off')  # Turn off axis labels\n",
    "    axs[i].set_title(f\"predicted class {emotion_map[int(predictions[i])]}\")  # Add a title for each subplot\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, test_data, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.y.unique(return_counts=True), test_data.y.unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorDataset(TensorDataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\"\"\"\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x.numpy()) \n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(), \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(),  \n",
    "])\n",
    "\n",
    "\n",
    "train_dataset_test = CustomTensorDataset(tensors=(X_train_tensor, y_train_tensor), transform=data_transforms)\n",
    "test_dataset_test = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader_test = DataLoader(train_dataset_test, batch_size=8, shuffle=True)\n",
    "test_loader_test = DataLoader(test_dataset_test, batch_size=8, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/fer2013.csv\")\n",
    "\n",
    "img_array = df.pixels.apply(lambda x: np.array(x.split(' ')))\n",
    "img_array = np.stack(img_array, axis=0)\n",
    "img_array.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../model/model_1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
